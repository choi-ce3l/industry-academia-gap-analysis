{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Topic Modeling - academia\n",
    "- title, abstract만 갖고 토픽 모델링 진행\n",
    "## 전처리\n",
    "- NER로 사람 이름 체크 및 제거\n",
    "- 불용어로 Information System, Research, Study 제거\n",
    "- 각 키워드를 소문자 변환 후에 띄어쓰기 제거 및 복수형태 통일\n",
    "## 모델\n",
    "Num Topics =  1 → Coherence = 0.2463\n",
    "Num Topics =  2 → Coherence = 0.3622\n",
    "Num Topics =  3 → Coherence = 0.3344\n",
    "Num Topics =  4 → Coherence = 0.3213\n",
    "Num Topics =  5 → Coherence = 0.3441\n",
    "Num Topics =  6 → Coherence = 0.3295\n",
    "Num Topics =  7 → Coherence = 0.3725\n",
    "Num Topics =  8 → Coherence = 0.3573\n",
    "Num Topics =  9 → Coherence = 0.3487\n",
    "Num Topics = 10 → Coherence = 0.3393\n",
    "Num Topics = 11 → Coherence = 0.3501\n",
    "Num Topics = 12 → Coherence = 0.3446\n",
    "Num Topics = 13 → Coherence = 0.3465\n",
    "Num Topics = 14 → Coherence = 0.3394\n",
    "Num Topics = 15 → Coherence = 0.3569\n",
    "Num Topics = 16 → Coherence = 0.3518\n",
    "Num Topics = 17 → Coherence = 0.3518\n",
    "Num Topics = 18 → Coherence = 0.3420\n",
    "Num Topics = 19 → Coherence = 0.3515\n",
    "Num Topics = 20 → Coherence = 0.3407\n",
    "\n",
    "Optimal #Topics = 7 with coherence 0.3725\n",
    "\n",
    "## 결과\n",
    "\n",
    "=== Top 7 Topics ===\n",
    "Topic  0: system, data, model, learning, based, approach, paper, design, process, analysis\n",
    "Topic  1: worker, market, power, crisis, platform, work, impact, service, demand, job\n",
    "Topic  2: user, work, interaction, effect, identity, agent, perception, technology, influence, result\n",
    "Topic  3: platform, effect, medium, consumer, product, user, online, information, impact, content\n",
    "Topic  4: organization, technology, innovation, student, use, case, strategy, development, process, business\n",
    "Topic  5: review, design, knowledge, platform, ecosystem, project, literature, information, value, business\n",
    "Topic  6: data, model, privacy, health, based, information, using, risk, method, result\n",
    "\n",
    "Coherence (c_v) = 0.3725\n",
    "\n"
   ],
   "id": "4073c6bbc3829e98"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-08T01:34:33.380036Z",
     "start_time": "2025-08-08T01:34:33.133279Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "acad=pd.read_csv('data/03_journal_2023_2025_with_methods.csv')\n",
    "acad.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3209 entries, 0 to 3208\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         3209 non-null   object\n",
      " 1   date          3209 non-null   int64 \n",
      " 2   abstract      3209 non-null   object\n",
      " 3   keywords      3209 non-null   object\n",
      " 4   authors       3209 non-null   object\n",
      " 5   affiliations  3209 non-null   object\n",
      " 6   methods       3209 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 175.6+ KB\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:34:33.840264Z",
     "start_time": "2025-08-08T01:34:33.834004Z"
    }
   },
   "cell_type": "code",
   "source": "acad.head()",
   "id": "8bc4eda7cad3c737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               title  date  \\\n",
       "0  Digital “x”?Charting a Path for Digital-Themed...  2023   \n",
       "1  Law, Economics, and Privacy: Implications of G...  2023   \n",
       "2  Spoiled for Choice? Personalized Recommendatio...  2023   \n",
       "3  A Theory-Driven Deep Learning Method for Voice...  2023   \n",
       "4        The Decoy Effect and Recommendation Systems  2023   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  As of late, the use of “digital” as a qualifie...   \n",
       "1  Widespread abuse of internet users’ privacy on...   \n",
       "2  Online healthcare platforms provide users with...   \n",
       "3  As artificial intelligence and digitalization ...   \n",
       "4  In this paper, we explore the decoy effect in ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  digital x, IT x, digitalization, digitization,...   \n",
       "1  data protection regulation, government policy,...   \n",
       "2  personal health management, online healthcare ...   \n",
       "3  customer response prediction, voice chat, theo...   \n",
       "4  recommendation system, personalization, decoy ...   \n",
       "\n",
       "                                             authors affiliations  \\\n",
       "0  Abayomi Baiyere, Varun Grover, Kalle J. Lyytin...          ISR   \n",
       "1  Ram D. Gopal, Hooman Hidaji, Sule Nur Kutlu, R...          ISR   \n",
       "2  Tongxin Zhou, Yingfei Wang, Lu (Lucy) Yan, Yon...          ISR   \n",
       "3  Gang Chen, Shuaiyong Xiao, Chenghong Zhang, Hu...          ISR   \n",
       "4  Nasim Mousavi, Panagiotis Adamopoulos, Jesse B...          ISR   \n",
       "\n",
       "                                             methods  \n",
       "0  The methodology of this paper is qualitative. ...  \n",
       "1  The paper uses a stylized analytical model to ...  \n",
       "2  We propose a personalized recommendation frame...  \n",
       "3  We propose a theory-driven deep learning metho...  \n",
       "4  The methodology of this paper is to conduct a ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>methods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Digital “x”?Charting a Path for Digital-Themed...</td>\n",
       "      <td>2023</td>\n",
       "      <td>As of late, the use of “digital” as a qualifie...</td>\n",
       "      <td>digital x, IT x, digitalization, digitization,...</td>\n",
       "      <td>Abayomi Baiyere, Varun Grover, Kalle J. Lyytin...</td>\n",
       "      <td>ISR</td>\n",
       "      <td>The methodology of this paper is qualitative. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Law, Economics, and Privacy: Implications of G...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Widespread abuse of internet users’ privacy on...</td>\n",
       "      <td>data protection regulation, government policy,...</td>\n",
       "      <td>Ram D. Gopal, Hooman Hidaji, Sule Nur Kutlu, R...</td>\n",
       "      <td>ISR</td>\n",
       "      <td>The paper uses a stylized analytical model to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spoiled for Choice? Personalized Recommendatio...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Online healthcare platforms provide users with...</td>\n",
       "      <td>personal health management, online healthcare ...</td>\n",
       "      <td>Tongxin Zhou, Yingfei Wang, Lu (Lucy) Yan, Yon...</td>\n",
       "      <td>ISR</td>\n",
       "      <td>We propose a personalized recommendation frame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Theory-Driven Deep Learning Method for Voice...</td>\n",
       "      <td>2023</td>\n",
       "      <td>As artificial intelligence and digitalization ...</td>\n",
       "      <td>customer response prediction, voice chat, theo...</td>\n",
       "      <td>Gang Chen, Shuaiyong Xiao, Chenghong Zhang, Hu...</td>\n",
       "      <td>ISR</td>\n",
       "      <td>We propose a theory-driven deep learning metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Decoy Effect and Recommendation Systems</td>\n",
       "      <td>2023</td>\n",
       "      <td>In this paper, we explore the decoy effect in ...</td>\n",
       "      <td>recommendation system, personalization, decoy ...</td>\n",
       "      <td>Nasim Mousavi, Panagiotis Adamopoulos, Jesse B...</td>\n",
       "      <td>ISR</td>\n",
       "      <td>The methodology of this paper is to conduct a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:39:08.252166Z",
     "start_time": "2025-08-08T01:34:34.436565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install spacy nltk gensim\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "# ——— NLTK 리소스 다운로드 ———\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# ——— spaCy 로드 (NER 용) ———\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ——— 사용자 정의 불용어 ———\n",
    "CUSTOM_STOPWORDS = {'information system', 'research', 'study'}\n",
    "\n",
    "\n",
    "# ——— NLTK 리소스 다운로드 ———\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ——— spaCy 모델 로드 ———\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ——— 사용자 정의 불용어 (소문자/공백제거 후 기준) ———\n",
    "CUSTOM_STOPWORDS = {'information system','information','system', 'research', 'study'}\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) NaN 또는 리스트 처리\n",
    "    2) HTML 태그 및 수식 제거\n",
    "    3) 알파벳·공백 외 문자 제거 → 소문자화\n",
    "    4) 토큰화 → NLTK 불용어 & 길이 >2 필터\n",
    "    5) POS 태그 필터링 (명사·동사 계열)\n",
    "    6) Lemmatize\n",
    "    7) spaCy NER로 PERSON 엔터티 제거\n",
    "    8) 사용자 정의 불용어 제외\n",
    "    \"\"\"\n",
    "    # 1) Null / 리스트 처리\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # 2) HTML, LaTeX 수식 제거\n",
    "    if '<' in text and '>' in text:\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\\(.*?\\\\\\)\", \"\", text)\n",
    "\n",
    "    # 3) 알파벳·공백 외 제거 → 소문자\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text).lower()\n",
    "\n",
    "    # 4) 토큰화 → 불용어·길이 필터\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        w for w in word_tokenize(text)\n",
    "        if w not in stop_words and len(w) > 2\n",
    "    ]\n",
    "\n",
    "    # 5) POS 태깅 → 명사·동사 계열만\n",
    "    allowed = {'NN','NNS','NNP','NNPS','VB','VBD','VBG','VBN','VBP','VBZ'}\n",
    "    tokens = [w for w, pos in pos_tag(tokens) if pos in allowed]\n",
    "\n",
    "    # 6) 표제어 추출\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tokens = [lemm.lemmatize(w) for w in tokens]\n",
    "\n",
    "    # 7) spaCy NER 적용 → PERSON 제거\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    tokens = [tok.text for tok in doc if tok.ent_type_ != 'PERSON']\n",
    "\n",
    "    # 8) 사용자 정의 불용어 제거\n",
    "    return [w for w in tokens if w not in CUSTOM_STOPWORDS]\n",
    "\n",
    "\n",
    "# ——— 데이터 불러오기 ———\n",
    "# df = pd.read_csv('your_data.csv')  # title, abstract 컬럼 포함\n",
    "# 예시: title + abstract 합치기\n",
    "acad['text'] = acad['title'].fillna('') + ' ' + acad['abstract'].fillna('')+acad['keywords'].fillna('')\n",
    "\n",
    "# ——— 전처리 적용 ———\n",
    "texts = [preprocess_text(doc) for doc in acad['text']]\n",
    "\n",
    "# ——— Gensim Dictionary & Corpus 생성 ———\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# (선택) 너무 드문 단어/전체의 절반 이상 등장 단어 필터링\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(txt) for txt in texts]\n",
    "\n",
    "# ——— 최적 토픽 수 탐색 (1~20) ———\n",
    "coherence_scores = []\n",
    "for k in range(1, 21):\n",
    "    lda = LdaModel(corpus=corpus,\n",
    "                   id2word=dictionary,\n",
    "                   num_topics=k,\n",
    "                   random_state=42,\n",
    "                   passes=10)\n",
    "    cm = CoherenceModel(model=lda,\n",
    "                        texts=texts,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence='c_v')\n",
    "    score = cm.get_coherence()\n",
    "    coherence_scores.append((k, score))\n",
    "    print(f\"Num Topics = {k:2d} → Coherence = {score:.4f}\")\n",
    "\n",
    "# ——— 최적 토픽 수 출력 ———\n",
    "best_k, best_score = max(coherence_scores, key=lambda x: x[1])\n",
    "print(f\"\\nOptimal #Topics = {best_k} with coherence {best_score:.4f}\")"
   ],
   "id": "bcefaf7eb4656ac1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics =  1 → Coherence = 0.2144\n",
      "Num Topics =  2 → Coherence = 0.3410\n",
      "Num Topics =  3 → Coherence = 0.3175\n",
      "Num Topics =  4 → Coherence = 0.3187\n",
      "Num Topics =  5 → Coherence = 0.3260\n",
      "Num Topics =  6 → Coherence = 0.3173\n",
      "Num Topics =  7 → Coherence = 0.3025\n",
      "Num Topics =  8 → Coherence = 0.3564\n",
      "Num Topics =  9 → Coherence = 0.3417\n",
      "Num Topics = 10 → Coherence = 0.3439\n",
      "Num Topics = 11 → Coherence = 0.3397\n",
      "Num Topics = 12 → Coherence = 0.3269\n",
      "Num Topics = 13 → Coherence = 0.3446\n",
      "Num Topics = 14 → Coherence = 0.3545\n",
      "Num Topics = 15 → Coherence = 0.3433\n",
      "Num Topics = 16 → Coherence = 0.3414\n",
      "Num Topics = 17 → Coherence = 0.3505\n",
      "Num Topics = 18 → Coherence = 0.3413\n",
      "Num Topics = 19 → Coherence = 0.3349\n",
      "Num Topics = 20 → Coherence = 0.3513\n",
      "\n",
      "Optimal #Topics = 8 with coherence 0.3564\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:39:26.437925Z",
     "start_time": "2025-08-08T01:39:26.096180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install spacy nltk gensim beautifulsoup4\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "# ——— NLTK 리소스 다운로드 ———\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# ——— spaCy 로드 (NER 용) ———\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "CUSTOM_STOPWORDS = {'information system','information','system', 'research', 'study'}\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) NaN 또는 리스트 처리\n",
    "    2) HTML 태그 및 수식 제거\n",
    "    3) 알파벳·공백 외 문자 제거 → 소문자화\n",
    "    4) 토큰화 → NLTK 불용어 & 길이 >2 필터\n",
    "    5) POS 태그 필터링 (명사·동사 계열)\n",
    "    6) Lemmatize\n",
    "    7) spaCy NER로 PERSON 엔터티 제거\n",
    "    8) 사용자 정의 불용어 제외\n",
    "    \"\"\"\n",
    "    # 1) Null / 리스트 처리\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # 2) HTML, LaTeX 수식 제거\n",
    "    if '<' in text and '>' in text:\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\\(.*?\\\\\\)\", \"\", text)\n",
    "\n",
    "    # 3) 알파벳·공백 외 제거 → 소문자\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text).lower()\n",
    "\n",
    "    # 4) 토큰화 → 불용어·길이 필터\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        w for w in word_tokenize(text)\n",
    "        if w not in stop_words and len(w) > 2\n",
    "    ]\n",
    "\n",
    "    # 5) POS 태깅 → 명사·동사 계열만\n",
    "    allowed = {'NN','NNS','NNP','NNPS','VB','VBD','VBG','VBN','VBP','VBZ'}\n",
    "    tokens = [w for w, pos in pos_tag(tokens) if pos in allowed]\n",
    "\n",
    "    # 6) 표제어 추출\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tokens = [lemm.lemmatize(w) for w in tokens]\n",
    "\n",
    "    # 7) spaCy NER 적용 → PERSON 제거\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    tokens = [tok.text for tok in doc if tok.ent_type_ != 'PERSON']\n",
    "\n",
    "    # 8) 사용자 정의 불용어 제거\n",
    "    return [w for w in tokens if w not in CUSTOM_STOPWORDS]\n",
    "\n",
    "\n",
    "def run_lda_pipeline(\n",
    "    df: pd.DataFrame,\n",
    "    text_cols: list[str] = ['title', 'abstract','keywords'],\n",
    "    num_topics: int = 8, # 토픽 개수 설정해야함\n",
    "    no_below: int = 5,\n",
    "    no_above: float = 0.5,\n",
    "    passes: int = 10\n",
    ") -> tuple[LdaModel, corpora.Dictionary, list, float]:\n",
    "    \"\"\"\n",
    "    1) df[text_cols] 합치고 preprocess_text 적용\n",
    "    2) Dictionary & Corpus 생성 (filter_extremes)\n",
    "    3) LDA 학습(num_topics)\n",
    "    4) 토픽 키워드 출력 & coherence 계산\n",
    "    Returns: (lda_model, dictionary, corpus, coherence_score)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['__text'] = df[text_cols].fillna('').agg(' '.join, axis=1)\n",
    "    texts = [preprocess_text(doc) for doc in df['__text']]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [dictionary.doc2bow(txt) for txt in texts]\n",
    "\n",
    "    lda = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=passes\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== Top {num_topics} Topics ===\")\n",
    "    for tid, terms in lda.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "        print(f\"Topic {tid:2d}: {', '.join([t for t,_ in terms])}\")\n",
    "\n",
    "    cm = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coh = cm.get_coherence()\n",
    "    print(f\"\\nCoherence (c_v) = {coh:.4f}\")\n",
    "\n",
    "    return lda, dictionary, corpus, coh"
   ],
   "id": "eddc718fddd62fc6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:40:47.491016Z",
     "start_time": "2025-08-08T01:39:27.494576Z"
    }
   },
   "cell_type": "code",
   "source": "lda_model, dictionary, corpus, coherence = run_lda_pipeline(acad)",
   "id": "4bffec5dbc00b108",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 8 Topics ===\n",
      "Topic  0: data, model, learning, based, process, approach, business, method, decision, machine\n",
      "Topic  1: customer, review, service, consumer, product, effect, decision, impact, agent, city\n",
      "Topic  2: data, privacy, security, cybersecurity, network, risk, using, image, power, cyber\n",
      "Topic  3: work, technology, innovation, student, game, role, development, organization, finding, team\n",
      "Topic  4: platform, market, effect, sharing, network, consumer, product, impact, user, strategy\n",
      "Topic  5: firm, performance, effect, market, project, innovation, success, data, business, level\n",
      "Topic  6: design, technology, health, service, organization, use, healthcare, management, case, process\n",
      "Topic  7: medium, model, user, language, community, llm, analysis, using, result, content\n",
      "\n",
      "Coherence (c_v) = 0.3564\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T02:01:22.980852Z",
     "start_time": "2025-08-08T02:01:22.955592Z"
    }
   },
   "cell_type": "code",
   "source": "print(acad.value_counts(subset='date'))",
   "id": "145d6f10f4132e04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2024    1275\n",
      "2023    1210\n",
      "2025     724\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 모델 저장 및 distribution 저장",
   "id": "77a80754ebc5da0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:25:23.774964Z",
     "start_time": "2025-08-09T07:25:23.228675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "\n",
    "# 1. LDA 모델 저장\n",
    "# 학습이 끝난 lda_model 객체가 있다고 가정합니다.\n",
    "lda_model.save('acadmodel/lda_model.model')\n",
    "\n",
    "# 2. 문서-토픽 분포 추출\n",
    "# corpus: LDA에 입력했던 Bow/BOW-TFIDF 형태의 말뭉치 리스트\n",
    "# min_probability=0 으로 설정하면, 확률이 0인 토픽도 반환해줍니다.\n",
    "doc_topics = [\n",
    "    lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "    for doc in corpus\n",
    "]\n",
    "\n",
    "# 3. 분포를 DataFrame으로 변환\n",
    "# 각 문서별로 토픽 확률만 추출 → 리스트 of lists\n",
    "distribution = [\n",
    "    [prob for _, prob in topic_dist]\n",
    "    for topic_dist in doc_topics\n",
    "]\n",
    "\n",
    "# 컬럼명 생성 (예: 'Topic_0', 'Topic_1', ...)\n",
    "num_topics = lda_model.num_topics\n",
    "columns = [f'Topic_{i}' for i in range(num_topics)]\n",
    "\n",
    "df_dist = pd.DataFrame(distribution, columns=columns)\n",
    "\n",
    "# 4. CSV로 저장\n",
    "df_dist.to_csv('data/03_academia_lda_document_topic_distribution.csv', index=False)\n",
    "\n",
    "print(\"LDA 모델과 문서-토픽 분포 CSV 저장이 완료되었습니다.\")"
   ],
   "id": "88c4c55ee7f118a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA 모델과 문서-토픽 분포 CSV 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:25:28.169573Z",
     "start_time": "2025-08-09T07:25:28.139297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "\n",
    "# —————————————————————————————————————\n",
    "# (이전 코드로 얻은) document–topic distribution DataFrame: df_dist\n",
    "# 컬럼명은 'Topic_0', 'Topic_1', … 'Topic_{num_topics-1}'\n",
    "# —————————————————————————————————————\n",
    "\n",
    "# 1) 제외할 토픽 번호 리스트 지정\n",
    "# 예시: 토픽 1, 9, 17을 제외하고 싶으면\n",
    "topics_to_exclude = []\n",
    "\n",
    "# 2) 제외할 토픽을 뺀 컬럼 리스트 생성\n",
    "remaining_cols = [\n",
    "    col for col in df_dist.columns\n",
    "    if int(col.split('_')[1]) not in topics_to_exclude\n",
    "]\n",
    "\n",
    "# 3) 제외된 컬럼들 중 가장 확률이 높은 토픽 이름(예: 'Topic_5')을 찾아서,\n",
    "#    숫자 부분만 정수로 추출해 'dominant_topic_excl' 컬럼에 저장\n",
    "df_dist['dominant_topic_excl'] = (\n",
    "    df_dist[remaining_cols]\n",
    "      .idxmax(axis=1)\n",
    "      .apply(lambda x: int(x.split('_')[1]))\n",
    ")\n",
    "\n",
    "# 4) 결과 확인\n",
    "print(df_dist[['dominant_topic_excl']].head())"
   ],
   "id": "8864afb3dbe08c6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dominant_topic_excl\n",
      "0                    3\n",
      "1                    4\n",
      "2                    6\n",
      "3                    1\n",
      "4                    1\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:25:30.700537Z",
     "start_time": "2025-08-09T07:25:30.688238Z"
    }
   },
   "cell_type": "code",
   "source": "print(df_dist.value_counts(subset=['dominant_topic_excl'],sort=False))",
   "id": "3289f0add9e5847c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominant_topic_excl\n",
      "0                      688\n",
      "1                      197\n",
      "2                      325\n",
      "3                      649\n",
      "4                      283\n",
      "5                      161\n",
      "6                      606\n",
      "7                      300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:26:27.730406Z",
     "start_time": "2025-08-09T07:26:27.710392Z"
    }
   },
   "cell_type": "code",
   "source": "df_dist.head()",
   "id": "1a665a66ab719693",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Topic_0   Topic_1   Topic_2   Topic_3   Topic_4   Topic_5   Topic_6  \\\n",
       "0  0.157803  0.001509  0.001509  0.553391  0.001509  0.001509  0.281260   \n",
       "1  0.001204  0.001205  0.216162  0.001204  0.713332  0.001204  0.064485   \n",
       "2  0.150836  0.198215  0.051539  0.001118  0.080849  0.001119  0.402748   \n",
       "3  0.390330  0.508155  0.013061  0.084850  0.000901  0.000902  0.000901   \n",
       "4  0.064038  0.930137  0.000971  0.000971  0.000971  0.000971  0.000971   \n",
       "\n",
       "    Topic_7  dominant_topic_excl  \n",
       "0  0.001510                    3  \n",
       "1  0.001204                    4  \n",
       "2  0.113576                    6  \n",
       "3  0.000901                    1  \n",
       "4  0.000971                    1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_5</th>\n",
       "      <th>Topic_6</th>\n",
       "      <th>Topic_7</th>\n",
       "      <th>dominant_topic_excl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.157803</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.553391</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.281260</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.216162</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.713332</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.064485</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150836</td>\n",
       "      <td>0.198215</td>\n",
       "      <td>0.051539</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.080849</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.402748</td>\n",
       "      <td>0.113576</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.390330</td>\n",
       "      <td>0.508155</td>\n",
       "      <td>0.013061</td>\n",
       "      <td>0.084850</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064038</td>\n",
       "      <td>0.930137</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:29:37.049274Z",
     "start_time": "2025-08-09T07:29:36.869699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# === 파일 경로 ===\n",
    "path_orig = \"data/03_academia_2023_2025_with_methods.csv\"                      # 원본\n",
    "path_dist = \"data/03_academia_lda_document_topic_distribution.csv\"  # LDA 분포\n",
    "out_path  = \"data/topic_academia/academia_with_dominant_topic.csv\"\n",
    "\n",
    "# === 로드 ===\n",
    "orig = pd.read_csv(path_orig, low_memory=False)\n",
    "dist = pd.read_csv(path_dist, low_memory=False)\n",
    "\n",
    "# (옵션) content가 content_text로 되어 있으면 맞춰주기\n",
    "if \"content\" not in orig.columns and \"content_text\" in orig.columns:\n",
    "    orig = orig.rename(columns={\"content_text\": \"content\"})\n",
    "\n",
    "# 1) 원본에 dominant_topic이 이미 있으면 그대로 사용\n",
    "if \"dominant_topic\" not in orig.columns:\n",
    "    # 2) 없으면 분포파일에서 가져오거나 계산\n",
    "    if \"dominant_topic_excl\" in dist.columns:\n",
    "        dom_series = dist[\"dominant_topic_excl\"]\n",
    "    else:\n",
    "        # Topic_0, Topic_1 ... 형태에서 argmax로 dominant topic 계산\n",
    "        topic_cols = [c for c in dist.columns if re.match(r\"^Topic_\\d+$\", c)]\n",
    "        if not topic_cols:\n",
    "            raise ValueError(\"분포 파일에 Topic_0, Topic_1 ... 형식의 컬럼이 없습니다.\")\n",
    "        topic_arr = dist[topic_cols].to_numpy()\n",
    "        best_idx = topic_arr.argmax(axis=1)  # 각 행에서 최고 확률의 컬럼 인덱스\n",
    "        topic_nums = np.array([int(re.search(r\"\\d+\", c).group()) for c in topic_cols])\n",
    "        dom_series = pd.Series(topic_nums[best_idx], index=dist.index, name=\"dominant_topic\")\n",
    "\n",
    "    # 행 순서 기준으로 dominant_topic만 원본에 붙이기\n",
    "    orig[\"__rowid__\"] = range(len(orig))\n",
    "    dist[\"__rowid__\"] = range(len(dist))\n",
    "    tmp = pd.DataFrame({\"__rowid__\": dist[\"__rowid__\"], \"dominant_topic\": dom_series})\n",
    "    orig = orig.merge(tmp, on=\"__rowid__\", how=\"left\").drop(columns=\"__rowid__\")\n",
    "\n",
    "# 3) 최종 필요한 컬럼만 남기기\n",
    "wanted = [\"title\", \"abstract\", \"affiliations\", \"date\",\"keywords\", \"dominant_topic\"]\n",
    "existing = [c for c in wanted if c in orig.columns]\n",
    "missing  = [c for c in wanted if c not in orig.columns]\n",
    "\n",
    "final = orig[existing].copy()\n",
    "final.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"저장 완료: {out_path}  (rows={len(final)}, cols={len(final.columns)})\")\n",
    "if missing:\n",
    "    print(\"다음 컬럼은 데이터에 없어 제외되었습니다:\", missing)"
   ],
   "id": "8f698d51c0076436",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: data/topic_academia/academia_with_dominant_topic.csv  (rows=3209, cols=6)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:29:47.006808Z",
     "start_time": "2025-08-09T07:29:46.978878Z"
    }
   },
   "cell_type": "code",
   "source": "final.info()",
   "id": "6458b042ae340e34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3209 entries, 0 to 3208\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           3209 non-null   object\n",
      " 1   abstract        3209 non-null   object\n",
      " 2   affiliations    3209 non-null   object\n",
      " 3   date            3209 non-null   int64 \n",
      " 4   keywords        3209 non-null   object\n",
      " 5   dominant_topic  3209 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 150.6+ KB\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:34:56.420508Z",
     "start_time": "2025-08-09T07:34:56.321481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === 입력: 바로 'final'이 메모리에 없으면 직전 결과 파일에서 읽음 ===\n",
    "if 'final' not in globals():\n",
    "    final = pd.read_csv(\"data/topic_academia/merged_min_columns.csv\", low_memory=False)\n",
    "\n",
    "# 사용 컬럼 확인 (필수: dominant_topic)\n",
    "required = {\"title\", \"abstract\", \"affiliations\",\"date\", \"keywords\", \"dominant_topic\"}\n",
    "missing = [c for c in required if c not in final.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"다음 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "# 저장 경로 (이미 존재한다고 하셨으므로 생성 생략해도 되지만, 안전하게 유지)\n",
    "out_dir = \"data/topic_academia\"  # ← 요청 경로\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# dominant_topic이 있는 행만\n",
    "df_topics = final.dropna(subset=['dominant_topic']).copy()\n",
    "\n",
    "# 토픽 값 정리(숫자면 int로, 아니면 문자열 유지)\n",
    "def normalize_topic(x):\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "df_topics['__topic_norm__'] = df_topics['dominant_topic'].apply(normalize_topic)\n",
    "\n",
    "# 파일명 안전하게 만드는 함수\n",
    "def safe_name(s):\n",
    "    return re.sub(r'[^0-9A-Za-z._-]+', '_', str(s))\n",
    "\n",
    "# 토픽별 저장\n",
    "for t, g in df_topics.groupby('__topic_norm__', sort=True):\n",
    "    fname = f\"dominant_topic_{safe_name(t)}.csv\"\n",
    "    out_path = os.path.join(out_dir, fname)\n",
    "    g.drop(columns=['__topic_norm__']).to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved: {out_path} (rows={len(g)})\")"
   ],
   "id": "c17c1699c8afe552",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/topic_academia/dominant_topic_0.csv (rows=688)\n",
      "Saved: data/topic_academia/dominant_topic_1.csv (rows=197)\n",
      "Saved: data/topic_academia/dominant_topic_2.csv (rows=325)\n",
      "Saved: data/topic_academia/dominant_topic_3.csv (rows=649)\n",
      "Saved: data/topic_academia/dominant_topic_4.csv (rows=283)\n",
      "Saved: data/topic_academia/dominant_topic_5.csv (rows=161)\n",
      "Saved: data/topic_academia/dominant_topic_6.csv (rows=606)\n",
      "Saved: data/topic_academia/dominant_topic_7.csv (rows=300)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f5d518e5e51244be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
