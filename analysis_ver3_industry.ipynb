{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Topic Modeling - academia\n",
    "- title, abstract만 갖고 토픽 모델링 진행\n",
    "## 전처리\n",
    "- NER로 사람 이름 체크 및 제거\n",
    "- 불용어로 Information System, Research, Study 제거\n",
    "- 각 키워드를 소문자 변환 후에 띄어쓰기 제거 및 복수형태 통일\n",
    "## 모델\n",
    "- coherence score 체크해서 최적 토픽 개수 찾기 1-20까지\n"
   ],
   "id": "4073c6bbc3829e98"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-08T05:29:41.736168Z",
     "start_time": "2025-08-08T05:29:41.068770Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "indu=pd.read_csv('data/05_article_2023_2025.csv')\n",
    "indu.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11672 entries, 0 to 11671\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title         11672 non-null  object \n",
      " 1   content       11672 non-null  object \n",
      " 2   date          11672 non-null  float64\n",
      " 3   affiliations  11672 non-null  object \n",
      " 4   keywords      6252 non-null   object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 456.1+ KB\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T05:29:41.826571Z",
     "start_time": "2025-08-08T05:29:41.816855Z"
    }
   },
   "cell_type": "code",
   "source": "indu.head()",
   "id": "8bc4eda7cad3c737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "0  ArtificialIntelligence(A Special Report) --- H...   \n",
       "1  ArtificialIntelligence(A Special Report) --- T...   \n",
       "2  ArtificialIntelligence(A Special Report) --- F...   \n",
       "3  Crunchbase UsesArtificialIntelligenceTo Predic...   \n",
       "4  On the Clock: Bosses' Mental Fitness Set for A...   \n",
       "\n",
       "                                             content    date  \\\n",
       "0  The current generation of college students is ...  2024.0   \n",
       "1  ChatGPT is barely two years old. And yet it's ...  2024.0   \n",
       "2  The race for AI dominance launched a stampede ...  2025.0   \n",
       "3  Crunchbase, the firm best known for its startu...  2025.0   \n",
       "4  Bosses already live in fear that a verbal miss...  2024.0   \n",
       "\n",
       "          affiliations keywords  \n",
       "0  Wall Street Journal      NaN  \n",
       "1  Wall Street Journal      NaN  \n",
       "2  Wall Street Journal      NaN  \n",
       "3  Wall Street Journal      NaN  \n",
       "4  Wall Street Journal      NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- H...</td>\n",
       "      <td>The current generation of college students is ...</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- T...</td>\n",
       "      <td>ChatGPT is barely two years old. And yet it's ...</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ArtificialIntelligence(A Special Report) --- F...</td>\n",
       "      <td>The race for AI dominance launched a stampede ...</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crunchbase UsesArtificialIntelligenceTo Predic...</td>\n",
       "      <td>Crunchbase, the firm best known for its startu...</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the Clock: Bosses' Mental Fitness Set for A...</td>\n",
       "      <td>Bosses already live in fear that a verbal miss...</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T06:31:27.989614Z",
     "start_time": "2025-08-08T06:11:48.958007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install spacy nltk gensim\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "# ——— NLTK 리소스 다운로드 ———\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# ——— spaCy 로드 (NER 용) ———\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ——— 사용자 정의 불용어 ———\n",
    "CUSTOM_STOPWORDS = {'informationsystem', 'research', 'study'}\n",
    "\n",
    "\n",
    "# ——— NLTK 리소스 다운로드 ———\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ——— spaCy 모델 로드 ———\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ——— 사용자 정의 불용어 (소문자/공백제거 후 기준) ———\n",
    "CUSTOM_STOPWORDS = {'informationsystem', 'research', 'study','musk','altman','trump','elon','grok'}\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) NaN 또는 리스트 처리\n",
    "    2) HTML 태그 및 수식 제거\n",
    "    3) 알파벳·공백 외 문자 제거 → 소문자화\n",
    "    4) 토큰화 → NLTK 불용어 & 길이 >2 필터\n",
    "    5) POS 태그 필터링 (명사·동사 계열)\n",
    "    6) Lemmatize\n",
    "    7) spaCy NER로 PERSON 엔터티 제거\n",
    "    8) 사용자 정의 불용어 제외\n",
    "    \"\"\"\n",
    "    # 1) Null / 리스트 처리\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # 2) HTML, LaTeX 수식 제거\n",
    "    if '<' in text and '>' in text:\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\\(.*?\\\\\\)\", \"\", text)\n",
    "\n",
    "    # 3) 알파벳·공백 외 제거 → 소문자\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text).lower()\n",
    "\n",
    "    # 4) 토큰화 → 불용어·길이 필터\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        w for w in word_tokenize(text)\n",
    "        if w not in stop_words and len(w) > 2\n",
    "    ]\n",
    "\n",
    "    # 5) POS 태깅 → 명사·동사 계열만\n",
    "    allowed = {'NN','NNS','NNP','NNPS'}\n",
    "    tokens = [w for w, pos in pos_tag(tokens) if pos in allowed]\n",
    "\n",
    "    # 6) 표제어 추출\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tokens = [lemm.lemmatize(w) for w in tokens]\n",
    "\n",
    "    # 7) spaCy NER 적용 → PERSON 제거\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    tokens = [tok.text for tok in doc if tok.ent_type_ != 'PERSON']\n",
    "\n",
    "    # 8) 사용자 정의 불용어 제거\n",
    "    return [w for w in tokens if w not in CUSTOM_STOPWORDS]\n",
    "\n",
    "\n",
    "# ——— 데이터 불러오기 ———\n",
    "# df = pd.read_csv('your_data.csv')  # title, abstract 컬럼 포함\n",
    "# 예시: title + abstract 합치기\n",
    "indu['text'] = indu['title'].fillna('') + ' ' + indu['content'].fillna('')\n",
    "\n",
    "# ——— 전처리 적용 ———\n",
    "texts = [preprocess_text(doc) for doc in indu['text']]\n",
    "\n",
    "# ——— Gensim Dictionary & Corpus 생성 ———\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# (선택) 너무 드문 단어/전체의 절반 이상 등장 단어 필터링\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(txt) for txt in texts]\n",
    "\n",
    "# ——— 최적 토픽 수 탐색 (1~20) ———\n",
    "coherence_scores = []\n",
    "for k in range(1, 21):\n",
    "    lda = LdaModel(corpus=corpus,\n",
    "                   id2word=dictionary,\n",
    "                   num_topics=k,\n",
    "                   random_state=42,\n",
    "                   passes=10)\n",
    "    cm = CoherenceModel(model=lda,\n",
    "                        texts=texts,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence='c_v')\n",
    "    score = cm.get_coherence()\n",
    "    coherence_scores.append((k, score))\n",
    "    print(f\"Num Topics = {k:2d} → Coherence = {score:.4f}\")\n",
    "\n",
    "# ——— 최적 토픽 수 출력 ———\n",
    "best_k, best_score = max(coherence_scores, key=lambda x: x[1])\n",
    "print(f\"\\nOptimal #Topics = {best_k} with coherence {best_score:.4f}\")"
   ],
   "id": "bcefaf7eb4656ac1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics =  1 → Coherence = 0.2888\n",
      "Num Topics =  2 → Coherence = 0.3500\n",
      "Num Topics =  3 → Coherence = 0.4138\n",
      "Num Topics =  4 → Coherence = 0.4109\n",
      "Num Topics =  5 → Coherence = 0.4481\n",
      "Num Topics =  6 → Coherence = 0.4465\n",
      "Num Topics =  7 → Coherence = 0.4726\n",
      "Num Topics =  8 → Coherence = 0.4724\n",
      "Num Topics =  9 → Coherence = 0.4691\n",
      "Num Topics = 10 → Coherence = 0.4568\n",
      "Num Topics = 11 → Coherence = 0.4690\n",
      "Num Topics = 12 → Coherence = 0.4609\n",
      "Num Topics = 13 → Coherence = 0.4790\n",
      "Num Topics = 14 → Coherence = 0.4802\n",
      "Num Topics = 15 → Coherence = 0.4912\n",
      "Num Topics = 16 → Coherence = 0.4967\n",
      "Num Topics = 17 → Coherence = 0.4852\n",
      "Num Topics = 18 → Coherence = 0.5094\n",
      "Num Topics = 19 → Coherence = 0.5183\n",
      "Num Topics = 20 → Coherence = 0.5140\n",
      "\n",
      "Optimal #Topics = 19 with coherence 0.5183\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T06:31:43.905879Z",
     "start_time": "2025-08-08T06:31:43.420878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install spacy nltk gensim beautifulsoup4\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "# ——— NLTK 리소스 다운로드 ———\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# ——— spaCy 로드 (NER 용) ———\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ——— 사용자 정의 불용어 ———\n",
    "CUSTOM_STOPWORDS = {'informationsystem', 'research', 'study','musk','altman','trump','elon','grok'}\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) NaN 또는 리스트 처리\n",
    "    2) HTML 태그 및 수식 제거\n",
    "    3) 알파벳·공백 외 문자 제거 → 소문자화\n",
    "    4) 토큰화 → NLTK 불용어 & 길이 >2 필터\n",
    "    5) POS 태그 필터링 (명사·동사 계열)\n",
    "    6) Lemmatize\n",
    "    7) spaCy NER로 PERSON 엔터티 제거\n",
    "    8) 사용자 정의 불용어 제외\n",
    "    \"\"\"\n",
    "    # 1) Null / 리스트 처리\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # 2) HTML, LaTeX 수식 제거\n",
    "    if '<' in text and '>' in text:\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"\\$.*?\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\\(.*?\\\\\\)\", \"\", text)\n",
    "\n",
    "    # 3) 알파벳·공백 외 제거 → 소문자\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text).lower()\n",
    "\n",
    "    # 4) 토큰화 → 불용어·길이 필터\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [\n",
    "        w for w in word_tokenize(text)\n",
    "        if w not in stop_words and len(w) > 2\n",
    "    ]\n",
    "\n",
    "    # 5) POS 태깅 → 명사·동사 계열만\n",
    "    allowed = {'NN','NNS','NNP','NNPS'}\n",
    "    tokens = [w for w, pos in pos_tag(tokens) if pos in allowed]\n",
    "\n",
    "    # 6) 표제어 추출\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tokens = [lemm.lemmatize(w) for w in tokens]\n",
    "\n",
    "    # 7) spaCy NER 적용 → PERSON 제거\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    tokens = [tok.text for tok in doc if tok.ent_type_ != 'PERSON']\n",
    "\n",
    "    # 8) 사용자 정의 불용어 제거\n",
    "    return [w for w in tokens if w not in CUSTOM_STOPWORDS]\n",
    "\n",
    "\n",
    "def run_lda_pipeline(\n",
    "    df: pd.DataFrame,\n",
    "    text_cols: list[str] = ['title', 'content'],\n",
    "    num_topics: int = 19,\n",
    "    no_below: int = 20,\n",
    "    no_above: float = 0.5,\n",
    "    passes: int = 10\n",
    ") -> tuple[LdaModel, corpora.Dictionary, list, float]:\n",
    "    \"\"\"\n",
    "    1) df[text_cols] 합치고 preprocess_text 적용\n",
    "    2) Dictionary & Corpus 생성 (filter_extremes)\n",
    "    3) LDA 학습(num_topics)\n",
    "    4) 토픽 키워드 출력 & coherence 계산\n",
    "    Returns: (lda_model, dictionary, corpus, coherence_score)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['__text'] = df[text_cols].fillna('').agg(' '.join, axis=1)\n",
    "    texts = [preprocess_text(doc) for doc in df['__text']]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [dictionary.doc2bow(txt) for txt in texts]\n",
    "\n",
    "    lda = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=passes\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== Top {num_topics} Topics ===\")\n",
    "    for tid, terms in lda.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "        print(f\"Topic {tid:2d}: {', '.join([t for t,_ in terms])}\")\n",
    "\n",
    "    cm = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coh = cm.get_coherence()\n",
    "    print(f\"\\nCoherence (c_v) = {coh:.4f}\")\n",
    "\n",
    "    return lda, dictionary, corpus, coh"
   ],
   "id": "eddc718fddd62fc6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/choihj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T07:00:36.241716Z",
     "start_time": "2025-08-08T06:31:54.404796Z"
    }
   },
   "cell_type": "code",
   "source": "lda_model, dictionary, corpus, coherence = run_lda_pipeline(indu)",
   "id": "4bffec5dbc00b108",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 19 Topics ===\n",
      "Topic  0: customer, data, business, startup, product, platform, service, team, founder, venture\n",
      "Topic  1: robot, system, car, vehicle, technology, security, food, city, drone, robotics\n",
      "Topic  2: meta, medium, election, platform, account, campaign, post, user, facebook, content\n",
      "Topic  3: job, student, university, technology, work, school, worker, science, researcher, tool\n",
      "Topic  4: data, system, risk, technology, law, safety, use, information, tool, privacy\n",
      "Topic  5: data, energy, power, center, water, project, electricity, centre, plant, demand\n",
      "Topic  6: game, character, player, world, fan, video, week, sport, star, event\n",
      "Topic  7: openai, copilot, board, ceo, team, microsoft, employee, week, safety, month\n",
      "Topic  8: news, story, medium, publisher, article, perplexity, site, content, search, product\n",
      "Topic  9: image, tool, video, google, user, photo, feature, content, creator, generator\n",
      "Topic 10: thing, way, world, something, lot, work, kind, idea, question, day\n",
      "Topic 11: court, deal, case, lawsuit, firm, lawyer, law, judge, competition, google\n",
      "Topic 12: market, stock, share, price, investor, growth, business, rate, economy, investment\n",
      "Topic 13: voice, music, artist, film, actor, writer, industry, song, studio, work\n",
      "Topic 14: user, chatgpt, google, language, search, question, tool, information, answer, openai\n",
      "Topic 15: apple, feature, device, phone, app, google, assistant, user, iphone, camera\n",
      "Topic 16: child, health, bbc, patient, technology, police, cancer, care, council, help\n",
      "Topic 17: government, country, state, president, minister, technology, leader, administration, security, official\n",
      "Topic 18: chip, china, deepseek, nvidia, technology, industry, power, semiconductor, world, intel\n",
      "\n",
      "Coherence (c_v) = 0.5183\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T07:11:08.733849Z",
     "start_time": "2025-08-08T07:11:08.631386Z"
    }
   },
   "cell_type": "code",
   "source": "indu.info()",
   "id": "145d6f10f4132e04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11672 entries, 0 to 11671\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title         11672 non-null  object \n",
      " 1   content       11672 non-null  object \n",
      " 2   date          11672 non-null  float64\n",
      " 3   affiliations  11672 non-null  object \n",
      " 4   keywords      6252 non-null   object \n",
      " 5   text          11672 non-null  object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 547.3+ KB\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 모델 저장 및 distribution 저장",
   "id": "1dd056a06dbc5801"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:57:07.280240Z",
     "start_time": "2025-08-09T06:57:03.588196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "\n",
    "# 1. LDA 모델 저장\n",
    "# 학습이 끝난 lda_model 객체가 있다고 가정합니다.\n",
    "lda_model.save('indu/lda_model.model')\n",
    "\n",
    "# 2. 문서-토픽 분포 추출\n",
    "# corpus: LDA에 입력했던 Bow/BOW-TFIDF 형태의 말뭉치 리스트\n",
    "# min_probability=0 으로 설정하면, 확률이 0인 토픽도 반환해줍니다.\n",
    "doc_topics = [\n",
    "    lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "    for doc in corpus\n",
    "]\n",
    "\n",
    "# 3. 분포를 DataFrame으로 변환\n",
    "# 각 문서별로 토픽 확률만 추출 → 리스트 of lists\n",
    "distribution = [\n",
    "    [prob for _, prob in topic_dist]\n",
    "    for topic_dist in doc_topics\n",
    "]\n",
    "\n",
    "# 컬럼명 생성 (예: 'Topic_0', 'Topic_1', ...)\n",
    "num_topics = lda_model.num_topics\n",
    "columns = [f'Topic_{i}' for i in range(num_topics)]\n",
    "\n",
    "df_dist = pd.DataFrame(distribution, columns=columns)\n",
    "\n",
    "# 4. CSV로 저장\n",
    "df_dist.to_csv('data/05_industry_lda_document_topic_distribution.csv', index=False)\n",
    "\n",
    "print(\"LDA 모델과 문서-토픽 분포 CSV 저장이 완료되었습니다.\")"
   ],
   "id": "b8c121b1d664b934",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA 모델과 문서-토픽 분포 CSV 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:57:07.302199Z",
     "start_time": "2025-08-09T06:57:07.287714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "\n",
    "# —————————————————————————————————————\n",
    "# (이전 코드로 얻은) document–topic distribution DataFrame: df_dist\n",
    "# 컬럼명은 'Topic_0', 'Topic_1', … 'Topic_{num_topics-1}'\n",
    "# —————————————————————————————————————\n",
    "\n",
    "# 1) 제외할 토픽 번호 리스트 지정\n",
    "# 예시: 토픽 1, 9, 17을 제외하고 싶으면\n",
    "topics_to_exclude = [2, 7, 8,10,11,17,18,6,12]\n",
    "\n",
    "# 2) 제외할 토픽을 뺀 컬럼 리스트 생성\n",
    "remaining_cols = [\n",
    "    col for col in df_dist.columns\n",
    "    if int(col.split('_')[1]) not in topics_to_exclude\n",
    "]\n",
    "\n",
    "# 3) 제외된 컬럼들 중 가장 확률이 높은 토픽 이름(예: 'Topic_5')을 찾아서,\n",
    "#    숫자 부분만 정수로 추출해 'dominant_topic_excl' 컬럼에 저장\n",
    "df_dist['dominant_topic_excl'] = (\n",
    "    df_dist[remaining_cols]\n",
    "      .idxmax(axis=1)\n",
    "      .apply(lambda x: int(x.split('_')[1]))\n",
    ")\n",
    "\n",
    "# 4) 결과 확인\n",
    "print(df_dist[['dominant_topic_excl']].head())"
   ],
   "id": "e95ff3ca4072a3b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dominant_topic_excl\n",
      "0                    3\n",
      "1                   14\n",
      "2                    5\n",
      "3                    0\n",
      "4                   16\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:57:07.395215Z",
     "start_time": "2025-08-09T06:57:07.388004Z"
    }
   },
   "cell_type": "code",
   "source": "print(df_dist.value_counts(subset=['dominant_topic_excl'],sort=False))",
   "id": "89b66882c6549085",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominant_topic_excl\n",
      "0                      1650\n",
      "1                       592\n",
      "3                      1025\n",
      "4                      1651\n",
      "5                       663\n",
      "9                      1052\n",
      "13                      647\n",
      "14                     2360\n",
      "15                     1139\n",
      "16                      893\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:57:10.593216Z",
     "start_time": "2025-08-09T06:57:10.459869Z"
    }
   },
   "cell_type": "code",
   "source": "df_dist.to_csv('data/05_industry_lda_document_topic_distribution.csv', index=False)",
   "id": "1d7948144b74405a",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T06:57:11.537828Z",
     "start_time": "2025-08-09T06:57:11.526941Z"
    }
   },
   "cell_type": "code",
   "source": "df_dist.head()",
   "id": "a0e74fd12b915246",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Topic_0   Topic_1   Topic_2   Topic_3   Topic_4   Topic_5   Topic_6  \\\n",
       "0  0.050902  0.012399  0.000184  0.776838  0.000184  0.000184  0.024984   \n",
       "1  0.000164  0.061899  0.000164  0.000164  0.000164  0.026284  0.112314   \n",
       "2  0.000258  0.000258  0.000258  0.000258  0.000258  0.768430  0.000258   \n",
       "3  0.708301  0.000253  0.000253  0.000253  0.000253  0.000253  0.000253   \n",
       "4  0.000246  0.026555  0.036876  0.134522  0.000246  0.000246  0.000246   \n",
       "\n",
       "    Topic_7   Topic_8   Topic_9  Topic_10  Topic_11  Topic_12  Topic_13  \\\n",
       "0  0.000184  0.000184  0.000184  0.111138  0.000184  0.000184  0.021345   \n",
       "1  0.000164  0.078295  0.029320  0.335656  0.000164  0.040525  0.020994   \n",
       "2  0.008740  0.020093  0.041604  0.029010  0.000258  0.000258  0.000258   \n",
       "3  0.000253  0.032466  0.000253  0.000253  0.000253  0.083215  0.000253   \n",
       "4  0.173942  0.000246  0.050926  0.194634  0.000246  0.055463  0.076475   \n",
       "\n",
       "   Topic_14  Topic_15  Topic_16  Topic_17  Topic_18  dominant_topic_excl  \n",
       "0  0.000184  0.000184  0.000184  0.000184  0.000184                    3  \n",
       "1  0.250363  0.000164  0.042878  0.000164  0.000164                   14  \n",
       "2  0.000258  0.000258  0.000258  0.000258  0.128764                    5  \n",
       "3  0.156662  0.006727  0.000253  0.000253  0.009335                    0  \n",
       "4  0.073283  0.000246  0.175108  0.000246  0.000246                   16  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_5</th>\n",
       "      <th>Topic_6</th>\n",
       "      <th>Topic_7</th>\n",
       "      <th>Topic_8</th>\n",
       "      <th>Topic_9</th>\n",
       "      <th>Topic_10</th>\n",
       "      <th>Topic_11</th>\n",
       "      <th>Topic_12</th>\n",
       "      <th>Topic_13</th>\n",
       "      <th>Topic_14</th>\n",
       "      <th>Topic_15</th>\n",
       "      <th>Topic_16</th>\n",
       "      <th>Topic_17</th>\n",
       "      <th>Topic_18</th>\n",
       "      <th>dominant_topic_excl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050902</td>\n",
       "      <td>0.012399</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.776838</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.024984</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.111138</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.021345</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.061899</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.026284</td>\n",
       "      <td>0.112314</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.078295</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.335656</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.040525</td>\n",
       "      <td>0.020994</td>\n",
       "      <td>0.250363</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.042878</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.768430</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>0.020093</td>\n",
       "      <td>0.041604</td>\n",
       "      <td>0.029010</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.708301</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.032466</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.083215</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.156662</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.036876</td>\n",
       "      <td>0.134522</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.173942</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.050926</td>\n",
       "      <td>0.194634</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.055463</td>\n",
       "      <td>0.076475</td>\n",
       "      <td>0.073283</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.175108</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:29:04.824701Z",
     "start_time": "2025-08-09T07:29:03.752008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# === 파일 경로 ===\n",
    "path_orig = \"data/05_industry_2023_2025.csv\"                      # 원본\n",
    "path_dist = \"data/05_industry_lda_document_topic_distribution.csv\"  # LDA 분포\n",
    "out_path  = \"data/topic_industry/industry_with_dominant_topic.csv\"\n",
    "\n",
    "# === 로드 ===\n",
    "orig = pd.read_csv(path_orig, low_memory=False)\n",
    "dist = pd.read_csv(path_dist, low_memory=False)\n",
    "\n",
    "# (옵션) content가 content_text로 되어 있으면 맞춰주기\n",
    "if \"content\" not in orig.columns and \"content_text\" in orig.columns:\n",
    "    orig = orig.rename(columns={\"content_text\": \"content\"})\n",
    "\n",
    "# 1) 원본에 dominant_topic이 이미 있으면 그대로 사용\n",
    "if \"dominant_topic\" not in orig.columns:\n",
    "    # 2) 없으면 분포파일에서 가져오거나 계산\n",
    "    if \"dominant_topic_excl\" in dist.columns:\n",
    "        dom_series = dist[\"dominant_topic_excl\"]\n",
    "    else:\n",
    "        # Topic_0, Topic_1 ... 형태에서 argmax로 dominant topic 계산\n",
    "        topic_cols = [c for c in dist.columns if re.match(r\"^Topic_\\d+$\", c)]\n",
    "        if not topic_cols:\n",
    "            raise ValueError(\"분포 파일에 Topic_0, Topic_1 ... 형식의 컬럼이 없습니다.\")\n",
    "        topic_arr = dist[topic_cols].to_numpy()\n",
    "        best_idx = topic_arr.argmax(axis=1)  # 각 행에서 최고 확률의 컬럼 인덱스\n",
    "        topic_nums = np.array([int(re.search(r\"\\d+\", c).group()) for c in topic_cols])\n",
    "        dom_series = pd.Series(topic_nums[best_idx], index=dist.index, name=\"dominant_topic\")\n",
    "\n",
    "    # 행 순서 기준으로 dominant_topic만 원본에 붙이기\n",
    "    orig[\"__rowid__\"] = range(len(orig))\n",
    "    dist[\"__rowid__\"] = range(len(dist))\n",
    "    tmp = pd.DataFrame({\"__rowid__\": dist[\"__rowid__\"], \"dominant_topic\": dom_series})\n",
    "    orig = orig.merge(tmp, on=\"__rowid__\", how=\"left\").drop(columns=\"__rowid__\")\n",
    "\n",
    "# 3) 최종 필요한 컬럼만 남기기\n",
    "wanted = [\"title\", \"content\", \"affiliations\", \"date\",\"keywords\", \"dominant_topic\"]\n",
    "existing = [c for c in wanted if c in orig.columns]\n",
    "missing  = [c for c in wanted if c not in orig.columns]\n",
    "\n",
    "final = orig[existing].copy()\n",
    "final.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"저장 완료: {out_path}  (rows={len(final)}, cols={len(final.columns)})\")\n",
    "if missing:\n",
    "    print(\"다음 컬럼은 데이터에 없어 제외되었습니다:\", missing)"
   ],
   "id": "2cd68b2fc32ed32b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: data/topic_industry/industry_with_dominant_topic.csv  (rows=11672, cols=6)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:29:04.924928Z",
     "start_time": "2025-08-09T07:29:04.913444Z"
    }
   },
   "cell_type": "code",
   "source": "final.info()",
   "id": "f04d7a790b958aeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11672 entries, 0 to 11671\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           11672 non-null  object \n",
      " 1   content         11672 non-null  object \n",
      " 2   affiliations    11672 non-null  object \n",
      " 3   date            11672 non-null  float64\n",
      " 4   keywords        6252 non-null   object \n",
      " 5   dominant_topic  11672 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 547.3+ KB\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T07:34:14.015305Z",
     "start_time": "2025-08-09T07:34:13.385441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === 입력: 바로 'final'이 메모리에 없으면 직전 결과 파일에서 읽음 ===\n",
    "if 'final' not in globals():\n",
    "    final = pd.read_csv(\"data/topic_industry/merged_min_columns.csv\", low_memory=False)\n",
    "\n",
    "# 사용 컬럼 확인 (필수: dominant_topic)\n",
    "required = {\"title\", \"content\", \"affiliations\",\"date\", \"keywords\", \"dominant_topic\"}\n",
    "missing = [c for c in required if c not in final.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"다음 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "# 저장 경로 (이미 존재한다고 하셨으므로 생성 생략해도 되지만, 안전하게 유지)\n",
    "out_dir = \"data/topic_industry\"  # ← 요청 경로\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# dominant_topic이 있는 행만\n",
    "df_topics = final.dropna(subset=['dominant_topic']).copy()\n",
    "\n",
    "# 토픽 값 정리(숫자면 int로, 아니면 문자열 유지)\n",
    "def normalize_topic(x):\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "df_topics['__topic_norm__'] = df_topics['dominant_topic'].apply(normalize_topic)\n",
    "\n",
    "# 파일명 안전하게 만드는 함수\n",
    "def safe_name(s):\n",
    "    return re.sub(r'[^0-9A-Za-z._-]+', '_', str(s))\n",
    "\n",
    "# 토픽별 저장\n",
    "for t, g in df_topics.groupby('__topic_norm__', sort=True):\n",
    "    fname = f\"dominant_topic_{safe_name(t)}.csv\"\n",
    "    out_path = os.path.join(out_dir, fname)\n",
    "    g.drop(columns=['__topic_norm__']).to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved: {out_path} (rows={len(g)})\")"
   ],
   "id": "7ffbead6d175a80d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/topic_industry/dominant_topic_0.csv (rows=1650)\n",
      "Saved: data/topic_industry/dominant_topic_1.csv (rows=592)\n",
      "Saved: data/topic_industry/dominant_topic_3.csv (rows=1025)\n",
      "Saved: data/topic_industry/dominant_topic_4.csv (rows=1651)\n",
      "Saved: data/topic_industry/dominant_topic_5.csv (rows=663)\n",
      "Saved: data/topic_industry/dominant_topic_9.csv (rows=1052)\n",
      "Saved: data/topic_industry/dominant_topic_13.csv (rows=647)\n",
      "Saved: data/topic_industry/dominant_topic_14.csv (rows=2360)\n",
      "Saved: data/topic_industry/dominant_topic_15.csv (rows=1139)\n",
      "Saved: data/topic_industry/dominant_topic_16.csv (rows=893)\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8bc4d381ec602486"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
